from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "EleutherAI/polyglot-ko-5.8b"

print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
instruction = "3ì„¸ ìœ ì•„ì—ê²Œ ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ì±…ì„ ì•Œë ¤ì¤˜"
inputs = tokenizer(instruction, return_tensors="pt").to(model.device)

with torch.no_grad():
    output = model.generate(
        **inputs,
        max_new_tokens=50,
        temperature=0.7,
        top_p=0.95
    )

print("ğŸ§  ëª¨ë¸ ì‘ë‹µ:")
print(tokenizer.decode(output[0], skip_special_tokens=True))
