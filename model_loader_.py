from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "EleutherAI/polyglot-ko-5.8b"

print("ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
instruction = "3ì„¸ ìœ ì•„ì—ê²Œ ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ì±…ì„ ì•Œë ¤ì¤˜"
inputs = tokenizer(instruction, return_tensors="pt").to(model.device)

with torch.no_grad():
    output = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=50,
        temperature=0.7,
        top_p=0.95,
        do_sample=True  # ì´ê±° ì•ˆ ë„£ìœ¼ë©´ ê²½ê³  ëœ¸ (sampling ê´€ë ¨)
    )

print("ğŸ§  ëª¨ë¸ ì‘ë‹µ:")
print(tokenizer.decode(output[0], skip_special_tokens=True))
